{"data": [{"semester": "Fall", "year": "2021", "date": "2021-09-16", "speaker": "Trapit Bansal", "website": "https://trapitbansal.com", "title": "Few-Shot Natural Language Processing via Self-Supervised Meta-Learning", "affiliation": "UMass", "sponsor": "Oracle Labs", "video": "", "abstract": "Humans show a remarkable capability to accurately solve a wide range of problems efficiently -- utilizing a limited amount of computation and experience. Deep learning models, by stark contrast, can be trained to be highly accurate on a narrow task while being highly inefficient in terms of the amount of compute and data required to reach that accuracy. Few-shot learning considers this problem of learning models that generalize to new tasks with very little supervision. Natural language processing (NLP) has seen recent breakthroughs with unsupervised pre-training of large models that can be applied to many NLP tasks, however, few-shot learning of new tasks is still inefficient. In this talk, I will present a sequence of work on meta-learning for improving few-shot learning of NLP tasks. Meta-learning, or learning to learn, treats the learning process itself as a learning problem from data, to learn systems that can generalize to new tasks efficiently. However, meta-learning requires a distribution over tasks with relevant labeled data that can be difficult to obtain, severely limiting the practical utility of meta-learning methods. I will present solutions that construct task distributions from unlabeled text data to enable large-scale meta-learning. The resulting self-supervised meta-learning methods optimize the pre-training directly for future fine-tuning with few examples, which leads to improved few-shot learning of new tasks. By providing useful training tasks for meta-learning, these approaches help lift a pertinent bottleneck for training meta-learning methods and should enable many future applications of meta-learning in NLP, such as hyper-parameter optimization, continual learning, neural architecture search, and more.", "bio": "Trapit is a Ph.D. student advised by Prof. Andrew McCallum at UMass Amherst. His recent research focuses on improving the generalization of natural language processing models with limited human-labeled data through meta-learning, self-supervised learning, and multi-task learning. In the past, he has also worked on machine learning methods for recommendation systems, information extraction, knowledge representation, and reinforcement learning for multi-agent systems. During his Ph.D., he has interned at Facebook, OpenAI, Google Research, and Microsoft Research. His work has also received a best paper award at ICLR 2018. Before starting his Ph.D., he obtained a B.S. and M.S. in Mathematics from the Indian Institute of Technology, Kanpur.", "area": "Meta-learning", "key": "TrapitBansal", "prettyDate": "September 16"}, {"semester": "Fall", "year": "2021", "date": "2021-09-23", "speaker": "Antonio Khalil Moretti", "website": "http://www.cs.columbia.edu/~amoretti/", "title": "Variational Combinatorial Sequential Monte Carlo Methods for Bayesian Phylogenetic Inference", "affiliation": "Columbia University", "sponsor": "Oracle Labs", "video": "", "abstract": "Bayesian phylogenetic inference is often conducted via local or sequential search over topologies and branch lengths using algorithms such as random-walk Markov chain Monte Carlo (MCMC) or Combinatorial Sequential Monte Carlo (CSMC). However, when MCMC is used for evolutionary parameter learning, convergence requires long runs with inefficient exploration of the state space. We introduce Variational Combinatorial Sequential Monte Carlo (VCSMC), a powerful framework that establishes variational sequential search to learn distributions over intricate combinatorial structures. We then develop nested CSMC, an efficient proposal distribution for CSMC and prove that nested CSMC is an exact approximation to the (intractable) locally optimal proposal. We use nested CSMC to define a second objective, VNCSMC which yields tighter lower bounds than VCSMC. We show that VCSMC and VNCSMC are computationally efficient and explore higher probability spaces than existing methods on a range of tasks.  ", "bio": "Antonio is currently a senior data scientist on the search algorithm team at Walmart Labs using machine learning to improve customer experience. Antonio recently completed a PhD in the Computer Science Department at Columbia University in the lab of Itsik Pe'er. He has developed a number of variational Bayesian inference methods for open problems in computational biology. At the heart of these questions has been a curiosity about how information is naturally encoded or represented in living systems, through neuronal firing activity or through the molecular sequences comprising the genome. To address these questions, his research has focused on the development of expressive statistical methodologies along with tractable inference algorithms for fast approximate inference on structured sequential data.", "area": "", "key": "AntonioKhalilMoretti", "prettyDate": "September 23"}, {"semester": "Fall", "year": "2021", "date": "2021-09-30", "speaker": "Jean Honorio", "website": "https://www.cs.purdue.edu/homes/jhonorio/", "title": "Fair Sparse Regression with Clustering: An Invex Relaxation for a Combinatorial Problem", "affiliation": "Purdue ", "sponsor": "Oracle Labs", "video": "", "abstract": "We study the problem of fair sparse regression on a biased dataset where bias depends upon a hidden binary attribute. The presence of a hidden attribute adds an extra layer of complexity to the problem by combining sparse regression and clustering with unknown binary labels. The corresponding optimization problem is combinatorial but we propose a continuous relaxation, resulting in an invex optimization problem. To the best of our knowledge, this is the first invex relaxation for a combinatorial problem. We show that our method recovers the correct support of the regression parameter vector, as well as the exact value of the hidden attribute for each sample. The above theoretical guarantees hold as long as the number of samples is logarithmic in terms of the dimension of the regression parameter vector.  The result above serves as a gentle introduction to a unifying framework, which uses the power of continuous relaxations (beyond convexity), Karush-Kuhn-Tucker conditions, primal-dual certificates and concentration inequalities. This framework has allowed us to produce novel algorithms for several NP-hard combinatorial problems, such as learning Bayesian networks, graphical games, inference in structured prediction, and community detection.  ", "bio": "Jean Honorio is an Assistant Professor in the Computer Science Department at Purdue University, as well as in the Statistics Department (by courtesy). Prior to joining Purdue, Jean was a postdoctoral associate at MIT, working with Tommi Jaakkola. His Erd\u0151s number is 3. His work has been partially funded by NSF. He is an editorial board reviewer of JMLR, and has served as area chair of NeurIPS, senior PC member of IJCAI and AAAI, PC member of NeurIPS, ICML, AISTATS among other conferences and journals.", "area": "", "key": "JeanHonorio", "prettyDate": "September 30"}, {"semester": "Fall", "year": "2021", "date": "2021-10-07", "speaker": "Rose Yu", "website": "https://roseyu.com", "title": "Towards Generalizable Deep Dynamics Learning ", "affiliation": "UC San Diego", "sponsor": "Oracle Labs", "video": "", "abstract": "Deep learning holds great promise in accelerating the prediction of physical dynamics relative to numerical solvers. However, current deep learning models for dynamics forecasting struggle with generalization. They only work in a specific domain and fail when applied to systems with different parameters, external forces, or boundary conditions. In this talk, I will demonstrate our efforts in improving the generalization of deep learning for forecasting physical dynamics. I will introduce (1) Equivariant-Net: a model that is inherently equivariant to groups of symmetries and thus generalizes automatically across groups. (2) DyAd: a model-based meta-learning method which can generalize across heterogeneous domains with latent task inference. I will showcase the advantage of our approaches on forecasting Rayleigh B\u00e9nard convection, real-world ocean currents, and temperatures.", "bio": "Dr. Rose Yu is an assistant professor at the University of California San Diego, Department of Computer Science and Engineering. She was a Postdoctoral Fellow at the California Institute of Technology.  Her research focuses on advancing machine learning techniques for large-scale spatiotemporal data analysis, with applications to sustainability, health, and physical sciences. A particular emphasis of her research is on physics-guided AI which aims to integrate first-principles with data-driven models. Among her awards, she has won Faculty Research Award from Facebook, Google, Amazon, and Adobe, Several Best Paper Awards, Best Dissertation Award in USC, and was nominated as one of the \u2019MIT Rising Stars in EECS\u2019. ", "area": "", "key": "RoseYu", "prettyDate": "October 07"}, {"semester": "Fall", "year": "2021", "date": "2021-10-14", "speaker": "Zhou Yu", "website": "http://www.cs.columbia.edu/~zhouyu/", "title": "TBA", "affiliation": "Columbia University", "sponsor": "Oracle Labs", "video": "", "abstract": "TBA", "bio": "TBA", "area": "NLP", "key": "ZhouYu", "prettyDate": "October 14"}, {"semester": "Fall", "year": "2021", "date": "2021-10-21", "speaker": "TBA", "website": "", "title": "TBA", "affiliation": "", "sponsor": "Oracle Labs", "video": "", "abstract": "TBA", "bio": "TBA", "area": "", "key": "TBA", "prettyDate": "October 21"}, {"semester": "Fall", "year": "2021", "date": "2021-10-28", "speaker": "TBA", "website": "", "title": "TBA", "affiliation": "", "sponsor": "Oracle Labs", "video": "", "abstract": "TBA", "bio": "TBA", "area": "", "key": "TBA", "prettyDate": "October 28"}, {"semester": "Fall", "year": "2021", "date": "2021-11-04", "speaker": "TBA", "website": "", "title": "TBA", "affiliation": "", "sponsor": "Oracle Labs", "video": "", "abstract": "TBA", "bio": "TBA", "area": "", "key": "TBA", "prettyDate": "November 04"}, {"semester": "Fall", "year": "2021", "date": "2021-11-18", "speaker": "Swetasudha Panda", "website": "https://swetapanda.github.io", "title": "TBA", "affiliation": "Oracle", "sponsor": "Oracle Labs", "video": "", "abstract": "TBA", "bio": "TBA", "area": "", "key": "SwetasudhaPanda", "prettyDate": "November 18"}, {"semester": "Fall", "year": "2021", "date": "2021-12-02", "speaker": "Jonathan Spencer", "website": "https://jspencer12.github.io", "title": "TBA", "affiliation": "", "sponsor": "Oracle Labs", "video": "", "abstract": "TBA", "bio": "TBA", "area": "Imitiation learning", "key": "JonathanSpencer", "prettyDate": "December 02"}]}