{"data": [{"semester": "Fall", "year": "2020", "date": "2020-09-24", "speaker": "John Wieting", "website": "https://www.cs.cmu.edu/~jwieting/", "title": "Learning and Applications of Paraphrastic Representations for Natural Language", "affiliation": "CMU", "sponsor": "Oracle Labs", "video": "https://www.youtube.com/watch?v=tQc1i4mgH-A", "abstract": "Representation learning has had a tremendous impact in machine learning and natural language processing (NLP), especially in recent years. Learned representations provide useful features needed for downstream tasks, allowing models to incorporate knowledge from billions of tokens of text. The result is better performance and generalization on many important problems of interest. This talk focuses on the problem of learning paraphrastic representations for units of language spanning from sub-words to full sentences \u2013 the latter being a focal point. Our primary goal is to learn models that can encode arbitrary word sequences into a vector with the property that sequences with similar semantics are near each other in the learned vector space, and that this property transfers across domains. We first show several simple, but effective, models to learn word and sentence representations on noisy paraphrases automatically extracted from bilingual corpora. These models outperform contemporary models on a variety of semantic evaluations. We then propose techniques to enable deep networks to learn effective semantic representations, addressing a limitation of our prior work. We also automatically construct a large paraphrase corpus that improves the performance of all our studied models, especially those using deep architectures, and has found uses for a variety of generation tasks such as paraphrase generation and style-transfer. We next propose models for multilingual paraphrastic sentence representations. Again, we first propose a simple and effective approach that outperforms more complicated methods on cross-lingual sentence similarity and mining bitext. We then propose a generative model that concentrates semantic information into a single interlingua representations and pushes information responsible for linguistic variation to separate language-specific representations. We show that this model has improved performance on both monolingual and cross-lingual tasks over prior work and successfully disentangles these two sources of information. Finally, we apply our representations to the task of fine-tuning neural machine translation systems using minimum risk training. The conventional approach is to use BLEU (Papineni et al., 2002), since that is commonly used for evaluation. However, we found that using an embedding model to evaluate similarity allows the range of possible scores to be continuous and, as a result, introduces fine-grained distinctions between similar translations. The result is better performance on both human evaluations and BLEU score, along with faster convergence during training.", "bio": "John Wieting is a recent PhD graduate of the Language Technology Institute at Carnegie Mellon University, where he was supervised by Graham Neubig and Taylor Berg-Kirkpatrick. He currently is a research scientist at Google Research. Previously he worked with Kevin Gimpel at the Toyota Technological Institute-Chicago, and completed his MS under the guidance of Dan Roth at the University of Illinois Urbana-Champaign. His research focuses on representation learning and its applications for natural language processing. He is also interested in language generation, with a particular interest in paraphrasing and related tasks.", "area": "NLP", "key": "JohnWieting", "prettyDate": "September 24"}, {"semester": "Fall", "year": "2020", "date": "2020-10-01", "speaker": "David Harwath", "website": "https://people.csail.mit.edu/dharwath/", "title": "Learning Spoken Language Through Vision", "affiliation": "MIT", "sponsor": "Oracle Labs", "video": "https://www.youtube.com/watch?v=xIr1W9eHr14", "abstract": "Humans learn spoken language and visual perception at an early age by being immersed in the world around them. Why can't computers do the same? In this talk, I will describe our work to develop methodologies for grounding continuous speech signals at the raw waveform level to natural image scenes. I will first present self-supervised models capable of jointly discovering spoken words and the visual objects to which they refer, all without conventional annotations in either modality. I will show how the representations learned by these models implicitly capture meaningful linguistic structure directly from the speech signal. Finally, I will demonstrate that these models can be applied across multiple languages, and that the visual domain can function as an \"interlingua,\" enabling the discovery of word-level semantic translations at the waveform level.", "bio": "David Harwath is an assistant professor in the computer science department at The University of Texas at Austin. Prior to joining UT, he was a research scientist in the Spoken Language Systems group at the MIT Computer Science and Artificial Intelligence Lab (CSAIL). His research focuses on multi-modal learning algorithms for speech, audio, vision, and text. His work has been published at venues such as NeurIPS, ACL, ICASSP, ECCV, and CVPR, and was nominated for a best paper award at ASRU 2015. Under the supervision of James Glass, his doctoral thesis introduced models for the joint perception of speech and vision. This work was awarded the 2018 George M. Sprowls Award for the best Ph.D. thesis in computer science at MIT. He holds a Ph.D. in computer science from MIT (2018), a S.M. in computer science from MIT (2013), and a\u00a0 B.S. in electrical engineering from UIUC (2010).", "area": "Multimodal perception", "key": "DavidHarwath", "prettyDate": "October 01"}, {"semester": "Fall", "year": "2020", "date": "2020-10-08", "speaker": "Yunzhu Li", "website": "https://people.csail.mit.edu/liyunzhu/", "title": "Learning-based Dynamics Modeling for Physical Inference and Model-based Control", "affiliation": "MIT", "sponsor": "Oracle Labs", "video": "https://www.youtube.com/watch?v=uFlL6cd-RLk", "abstract": "Humans have a strong intuitive understanding of physics. From visual observations, we can predict how the environment would change if we apply a specific action. This ability applies to objects of different materials, including rigid bodies, deformable objects, and fluids, which enables a tremendous amount of manipulation skills that are far beyond the reach of the current robot. It is desirable to help the robot learn from its interactions and better understand the dynamics. In this talk, I will present our attempt to model the dynamics of the environment from observational data by combining learning with different state representations and model class, aiming to capture the compositional nature of the objects and introduce the desired inductive bias. Using the learned simulators, robots can infer the physical properties and the relational structure within a dynamic environment and have achieved success in complex manipulation tasks, such as manipulating a pile of boxes, a cup of water, and a deformable foam. I will also present our work on building a scalable tactile glove to learn the patterns in human-object interaction and discuss how it can contribute to a multi-modal perception system that can facilitate human activities learning and dynamics understanding.", "bio": "Yunzhu Li is a Ph.D. student at MIT, advised by Prof. Antonio Torralba and Prof. Russ Tedrake. He works on the intersection of computer vision, machine learning, and robotics. He is particularly interested in enabling robots to better perceive and interact with the world via learning-based dynamics modeling and multimodal perception. Yunzhu is a recipient of the Adobe Research Fellowship. His research was published in top journals and conferences, including Nature, NeurIPS, CVPR, ICRA, etc., and has been featured by CNN, BBC, Forbes, The Economist, MIT Technology Review, WIRED, and other media outlets. Before coming to MIT, he received a B.S. degree from Peking University. He has also spent time at Stanford AI Lab and NVIDIA Robotics Research Lab.", "area": "Vision/Robotics", "key": "YunzhuLi", "prettyDate": "October 08"}, {"semester": "Fall", "year": "2020", "date": "2020-10-15", "speaker": "Fatemeh Mireshghallah", "website": "https://cseweb.ucsd.edu/~fmireshg/", "title": "Privacy and Fairness in Deep Neural Network Inference", "affiliation": "UCSD", "sponsor": "Oracle Labs", "video": "https://www.youtube.com/watch?v=nbFjjOHLvHQ", "abstract": "Cloud-based execution of machine learning models is becoming increasingly prevalent. Futuristic mixed or augmented reality and robotic applications rely on remote cloud localization services. Consumers also use these services to perform mundane everyday tasks such as simply applying filters on images, or calling smart home devices or virtual assistants. Their personal information is collected and sent to a service-provider on the cloud where a machine learning task is executed and the query is responded to. This execution model could have enormous privacy ramifications when consumers use it in their home or private industrial setup. In this talk, I will talk about two methods that we proposed (Shredder and Cloak) to address the privacy and latency issues in the aforementioned setup. These methods do not rely on encryption and aims to reduce the information content of the query with as little as possible compromise on the inference accuracy by adding noise to the query sent to the cloud. I will conclude the talk by discussing the effects that different privacy mitigations have on the fairness of the deployed models.", "bio": "Fatemehsadat Mireshghallah is a 3rd year CS Ph.D. student at UC San Diego. She received her B.Sc. in Computer Engineering with honors from Sharif University of Technology in 2018 and she is a recipient of the National Center for Women & IT (NCWIT) Aspirations in Computing Collegiate award in 2020, for her work on light-weight privacy-preserving ML. Her thesis research at UCSD is focused on designing practical and low-overhead privacy-preserving solutions for deployed cloud-based deep learning models. She was a research intern at Microsoft Research AI in summer 2020 where she worked on privacy-preserving text generation.", "area": "Privacy for ML", "key": "FatemehMireshghallah", "prettyDate": "October 15"}, {"semester": "Fall", "year": "2020", "date": "2020-10-22", "speaker": "Marcus Gualtieri", "website": "https://www.ccs.neu.edu/home/mgualti/", "title": "TBA", "affiliation": "Northeastern University", "sponsor": "Oracle Labs", "video": "", "abstract": "TBA", "bio": "TBA", "area": "Deep RL", "key": "MarcusGualtieri", "prettyDate": "October 22"}, {"semester": "Fall", "year": "2020", "date": "2020-10-29", "speaker": "Kalesha Bullard", "website": "https://www.kaleshabullard.com/", "title": "TBA", "affiliation": "FAIR", "sponsor": "Oracle Labs", "video": "", "abstract": "TBA", "bio": "TBA", "area": "Multi-Agent Reinforcement Learning", "key": "KaleshaBullard", "prettyDate": "October 29"}, {"semester": "Fall", "year": "2020", "date": "2020-11-05", "speaker": "Ankur Parikh", "website": "https://research.google/people/104995/", "title": "TBA", "affiliation": "Google", "sponsor": "Oracle Labs", "video": "", "abstract": "TBA", "bio": "TBA", "area": "NLP", "key": "AnkurParikh", "prettyDate": "November 05"}, {"semester": "Fall", "year": "2020", "date": "2020-11-19", "speaker": "Kelsey Allen", "website": "https://web.mit.edu/krallen/www/", "title": "TBA", "affiliation": "MIT", "sponsor": "Oracle Labs", "video": "", "abstract": "TBA", "bio": "TBA", "area": "Robotics", "key": "KelseyAllen", "prettyDate": "November 19"}, {"semester": "Fall", "year": "2020", "date": "2020-11-12", "speaker": "Maria De-Arteaga", "website": "https://mariadearteaga.com/", "title": "TBA", "affiliation": "UT Austin", "sponsor": "Oracle Labs", "video": "", "abstract": "TBA", "bio": "TBA", "area": "Human-centered ML", "key": "MariaDe-Arteaga", "prettyDate": "November 12"}]}