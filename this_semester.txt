{"data": [{"semester": "Spring", "year": "2021", "date": "2021-02-11", "speaker": "Tamara Broderick", "website": "https://people.csail.mit.edu/tbroderick/", "title": "An Automatic Finite-Sample Robustness Metric: Can Dropping a Little Data Change Conclusions?", "affiliation": "MIT", "sponsor": "Oracle Labs", "video": "https://www.youtube.com/watch?v=w8OX0lK1CKo", "abstract": "We propose a method to assess the sensitivity of data analyses to the removal of a small fraction of the data set. Analyzing all possible data subsets of a certain size is computationally prohibitive, so we provide a finite-data metric to approximately compute the number (or fraction) of observations that has the greatest influence on a given result when dropped. We call our resulting metric the Approximate Maximum Influence Perturbation. Our approximation is automatically computable and works for common estimators (including OLS, IV, GMM, MLE, and variational Bayes). We provide explicit finite-sample error bounds on our approximation for linear and instrumental variables regressions. At minimal computational cost, our metric provides an exact finite-sample lower bound on sensitivity for any estimator, so any non-robustness our metric finds is conclusive. We demonstrate that the Approximate Maximum Influence Perturbation is driven by the signal-to-noise ratio in the inference problem, is not reflected in standard errors, does not disappear asymptotically, and is not a product of misspecification. We focus on econometric analyses in our applications. Several empirical applications show that even 2-parameter linear regression analyses of randomized trials can be highly sensitive. While we find some applications are robust, in others the sign of a treatment effect can be changed by dropping less than 1% of the sample even when standard errors are small. ", "bio": "Tamara Broderick is an Associate Professor in the Department of Electrical Engineering and Computer Science at MIT. She is a member of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), the MIT Statistics and Data Science Center, and the Institute for Data, Systems, and Society (IDSS). She completed her Ph.D. in Statistics at the University of California, Berkeley in 2014. Previously, she received an AB in Mathematics from Princeton University (2007), a Master of Advanced Study for completion of Part III of the Mathematical Tripos from the University of Cambridge (2008), an MPhil by research in Physics from the University of Cambridge (2009), and an MS in Computer Science from the University of California, Berkeley (2013). Her recent research has focused on developing and analyzing models for scalable Bayesian machine learning. She has been awarded an Early Career Grant (ECG) from the Office of Naval Research (2020), an AISTATS Notable Paper Award (2019), an NSF CAREER Award (2018), a Sloan Research Fellowship (2018), an Army Research Office Young Investigator Program (YIP) award (2017), Google Faculty Research Awards, an Amazon Research Award, the ISBA Lifetime Members Junior Researcher Award, the Savage Award (for an outstanding doctoral dissertation in Bayesian theory and methods), the Evelyn Fix Memorial Medal and Citation (for the Ph.D. student on the Berkeley campus showing the greatest promise in statistical research), the Berkeley Fellowship, an NSF Graduate Research Fellowship, a Marshall Scholarship, and the Phi Beta Kappa Prize (for the graduating Princeton senior with the highest academic average).", "area": "ML Theory", "key": "TamaraBroderick", "prettyDate": "February 11"}, {"semester": "Spring", "year": "2021", "date": "2021-02-18", "speaker": "Vered Shwartz", "website": "https://vered1986.github.io/", "title": "Commonsense Knowledge and Reasoning in Natural Language", "affiliation": "AI2", "sponsor": "Oracle Labs", "video": "https://www.youtube.com/watch?v=0RGhK8_uQWo", "abstract": "Natural language understanding models are trained on a sample of the real-world situations they may encounter. Commonsense and world knowledge, language, and reasoning skills can help them address unknown situations sensibly.\u00a0 In this talk I will present two lines of work addressing commonsense knowledge and reasoning in natural language. I will first present a method for discovering relevant knowledge which is unstated but may be required for solving a particular problem, e.g., to correctly resolve \"Children need to eat more vegetables because they [children / vegetables] are healthy\" one needs to know that \"vegetables are healthy\". Such knowledge is discovered through a process of asking information seeking clarification questions (e.g. \"what is the purpose of vegetables?\") and answering them (\"to provide nutrients\"). I will then discuss nonmonotonic reasoning in natural language, a core human reasoning ability that has been studied in classical AI but mostly overlooked in modern NLP. I will talk about several recent papers addressing abductive reasoning (reasoning about plausible explanations), counterfactual reasoning (what if?) and defeasible reasoning (updating beliefs given additional information). Finally, I will discuss open problems in language, knowledge, and reasoning. ", "bio": "Vered Shwartz is a postdoctoral researcher at the Allen Institute for AI (AI2) and the Paul G. Allen School of Computer Science & Engineering at the University of Washington. Previously, she completed her PhD in Computer Science from Bar-Ilan University, under the supervision of Prof. Ido Dagan. Her research interests include commonsense reasoning, lexical and compositional semantics.", "area": "NLP", "key": "VeredShwartz", "prettyDate": "February 18"}, {"semester": "Spring", "year": "2021", "date": "2021-02-25", "speaker": "Samory Kpotufe", "website": "http://www.columbia.edu/~skk2175/", "title": "Some Recent Insights on Transfer-Learning", "affiliation": "Columbia University", "sponsor": "Oracle Labs", "video": "https://www.youtube.com/watch?v=1kpHC9HZXVQ", "abstract": "A common situation in Machine Learning is one where training data is not fully representative of a target population due to bias in the sampling mechanism or due to prohibitive target sampling costs. In such situations, we aim to \u2019transfer\u2019 relevant information from the training data (a.k.a. source data) to the target application. How much information is in the source data about the target application? Would some amount of target data improve transfer? These are all practical questions that depend crucially on 'how far' the source domain is from the target. However, how to properly measure 'distance' between source and target domains remains largely unclear. In this talk we will argue that much of the traditional notions of 'distance' (e.g. KL-divergence, extensions of TV such as D_A discrepancy, density-ratios, Wasserstein distance) can yield an over-pessimistic picture of transferability. Instead, we show that some new notions of 'relative dimension' between source and target (which we simply term 'transfer-exponents') capture a continuum from easy to hard transfer. Transfer-exponents uncover a rich set of situations where transfer is possible even at fast rates; they encode relative benefits of source and target samples, and have interesting implications for related problems such as 'multi-task or multi-source learning'. In particular, in the case of transfer from multiple sources, we will discuss (if time permits) a strong dichotomy between minimax and adaptive rates: no adaptive procedure exists that can achieve the same rates as minimax (oracle) procedures.\u00a0 The talk is based on earlier work with Guillaume Martinet, and ongoing work with Steve Hanneke.", "bio": "I graduated (Sept 2010) in Computer Science at the University of California, San Diego, advised by Sanjoy Dasgupta. I then was a researcher at the Max Planck Institute for Intelligent Systems. At the MPI I worked in the department of Bernhard Schoelkopf, in the learning theory group of Ulrike von Luxburg. Following this, I spent a couple years as an Assistant Research Professor at the Toyota Technological Institute at Chicago. I then spent 4 years at ORFE, Princeton University as an Assistant Professor. Recently I was a visiting member at the Institute of Advanced Study from January to July 2020.", "area": "ML Theory", "key": "SamoryKpotufe", "prettyDate": "February 25"}, {"semester": "Spring", "year": "2021", "date": "2021-03-04", "speaker": "Nima Hamidi", "website": "http://stanford.edu/~hamidi/", "title": "On Worst-case Regret of Linear Thompson Sampling", "affiliation": "Stanford University", "sponsor": "Oracle Labs", "video": "https://www.youtube.com/watch?v=VAET8L9mRZU", "abstract": "In this paper, we consider the worst-case regret of Linear Thompson Sampling (LinTS) for the linear bandit problem. Russo and Van Roy (2014) show that the Bayesian regret of LinTS is bounded above by $\\widetilde{\\mathcal{O}}(d\\sqrt{T})$ where $T$ is the time horizon and $d$ is the number of parameters. While this bound matches the minimax lower-bounds for this problem up to logarithmic factors, the existence of a similar worst-case regret bound is still unknown. The only known worst-case regret bound for LinTS, due to Agrawal and Goyal (2013b); Abeille et al. (2017), is $\\widetilde{\\mathcal{O}}(d\\sqrt{dT})$ which requires the posterior variance to be inflated by a factor of $\\widetilde{\\mathcal{O}}(\\sqrt{d})$. While this bound is far from the minimax optimal rate by a factor of $\\sqrt{d}$, in this paper we show that it is the best possible one can get, settling an open problem stated in Russo et al. (2018). Specifically, we construct examples to show that, without the inflation, LinTS can incur linear regret up to time $\\exp(\\mathcal{O}(d))$. We then demonstrate that, under mild conditions, a slightly modified version of LinTS requires only an $\\widetilde{\\mathcal{O}}(1)$ inflation where the constant depends on the diversity of the optimal arm.", "bio": "Nima Hamidi is a sixth year Ph.D. student in the Stanford Department of Statistics. He received a B.Sc. degree in Software Engineering and Pure Mathematics and a M.Sc. degree in Pure Mathematics from Sharif University. His research interests include multi-armed bandit experiments and low-rank matrix estimation.", "area": "ML Theory", "key": "NimaHamidi", "prettyDate": "March 04"}, {"semester": "Spring", "year": "2021", "date": "2021-03-11", "speaker": "Marinka Zitnik", "website": "https://dbmi.hms.harvard.edu/people/marinka-zitnik", "title": "Advances in graph neural networks and their applications to the development of therapeutics", "affiliation": "Harvard Medical School", "sponsor": "Oracle Labs", "video": "https://www.youtube.com/watch?v=NPLOM-dYGW8", "abstract": "The success of machine learning depends heavily on the choice of representations used for downstream tasks. Graph neural networks have emerged as a predominant choice for learning representations of networked data. In this talk, I describe our efforts to expand the scope and ease the applicability of graph representation learning. First, I outline SubGNN, a subgraph neural network for learning disentangled subgraph representations. Second, I will describe G-Meta, a novel meta-learning approach for graphs. G-Meta uses subgraphs to generalize to completely new graphs and never-before-seen labels using only a handful of nodes or edges. G-Meta is theoretically justified and scales to orders of magnitude larger datasets than prior work. Finally, I will discuss applications in the development of safe and effective therapeutics. The new methods have enabled the repurposing of drugs for emerging diseases, including COVID-19, where our predictions were experimentally verified in the wet laboratory. Further, our knowledge graph methods enabled discovering dozens of combinations of drugs safe for patients with considerably fewer unwanted side effects than today's treatments. Lastly, I describe our efforts in learning actionable representations that allow users of our models to receive predictions that can be interpreted meaningfully. ", "bio": "Marinka Zitnik (https://zitniklab.hms.harvard.edu) is an Assistant Professor at Harvard University with appointments in the Department of Biomedical Informatics, Broad Institute of MIT and Harvard, and Harvard Data Science. Dr. Zitnik is a computer scientist studying machine learning, focusing on challenges brought forward by data in science, medicine, and health. Before Harvard, she was a postdoctoral fellow in Computer Science at Stanford and also a member of the Chan Zuckerberg Biohub. Dr. Zitnik has published extensively in top ML venues (e.g., NeurIPS, ICLR, ICML) and leading interdisciplinary journals (e.g., Nature Methods, Nature Communications, PNAS). She has organized numerous workshops and tutorials in the nexus of AI, deep learning, drug discovery, and medical AI at leading conferences (NeurIPS, ICLR, ICML, ISMB, AAAI, WWW), where she is also in the organizing committees. She also organized the National Symposium on drugs for future pandemics on behalf of the NSF. Her research won Bayer Early Excellence in Science Award and numerous best paper and research awards from the International Society for Computational Biology. She was named a Rising Star in Electrical Engineering and Computer Science (EECS) by MIT and also a Next Generation in Biomedicine by Broad Institute of MIT and Harvard, being the only young scientist who received such recognition in both EECS and Biomedicine.", "area": "Biomedical Informatics", "key": "MarinkaZitnik", "prettyDate": "March 11"}, {"semester": "Spring", "year": "2021", "date": "2021-03-18", "speaker": "N/A", "website": "", "title": "Data Science Conference week", "affiliation": "", "sponsor": "Oracle Labs", "video": "", "abstract": "", "bio": "", "area": "", "key": "N/A", "prettyDate": "March 18"}, {"semester": "Spring", "year": "2021", "date": "2021-03-25", "speaker": "Gedas Bertasius", "website": "https://gberta.github.io/", "title": "Video Understanding with Modern Language Models", "affiliation": "Facebook AI", "sponsor": "Oracle Labs", "video": "https://www.youtube.com/watch?v=Apsw9HSh3EU", "abstract": "Humans understand the world by processing signals from both vision and language. Similarly, we believe that language understanding can be beneficial for developing better video understanding systems. In this talk, I will present several of our proposed video understanding frameworks that incorporate models from the language domain. First, I will introduce TimeSformer, the first convolution-free architecture for video modeling built exclusively with self-attention. It achieves the best reported numbers on major action recognition benchmarks, and it is also more efficient than the state-of-the-art 3D CNNs. Afterwards, I will present COBE, a new large-scale framework for learning contextualized object representations in settings involving human-object interactions. Our approach exploits automatically-transcribed speech narrations from instructional YouTube videos, and it does not require manual annotations. Lastly, I will introduce a multi-modal video-based text generation framework Vx2Text, which outperforms state-of-the-art on three video based text-generation tasks: captioning, question answering and dialoguing.", "bio": "Gedas Bertasius is a postdoctoral researcher at Facebook AI working on computer vision and machine learning problems. His current research focuses on topics of video understanding, first-person vision, and multi-modal deep learning. He received his Bachelors Degree in Computer Science from Dartmouth College, and a Ph.D. in Computer Science from the University of Pennsylvania. His recent work was nominated for the CVPR 2020 best paper award.", "area": "Vision", "key": "GedasBertasius", "prettyDate": "March 25"}, {"semester": "Spring", "year": "2021", "date": "2021-04-08", "speaker": "He He", "website": "https://hhexiy.github.io/", "title": "Guarding Against Spurious Correlations in Natural Language Understanding", "affiliation": "NYU", "sponsor": "Oracle Labs", "video": "https://www.youtube.com/watch?v=kogfp0WnGgA", "abstract": "While we have made great progress in natural language understanding, transferring the success from benchmark datasets to real applications has not always been smooth. Notably, models sometimes make mistakes that are confusing and unexpected to humans. In this talk, I will discuss shortcuts in NLP tasks and present our recent works on guarding against spurious correlations in natural language understanding tasks (e.g. textual entailment and paraphrase identification) from the perspectives of both robust learning algorithms and better data coverage. Motivated by the observation that our data often contains a small amount of \"unbiased\" examples that do not exhibit spurious correlations, we present new learning algorithms that better exploit these minority examples. On the other hand, we may want to directly augment such \"unbiased\" examples. While recent works along this line are promising, we show several pitfalls in the data augmentation approach.", "bio": "He He is an assistant professor in the Center for Data Science and Courant Institute at New York University. Before joining NYU, she spent a year at Amazon Web Services and was a postdoc at Stanford University. She received her PhD from University of Maryland, College Park. She is broadly interested in machine learning and natural language processing. Her current research interests include text generation, dialogue systems, and robust language understanding.", "area": "NLP", "key": "HeHe", "prettyDate": "April 08"}, {"semester": "Spring", "year": "2021", "date": "2021-04-15", "speaker": "Luciana Benotti", "website": "https://twitter.com/lucianabenotti?lang=enn", "title": "Two types of grounding in dialogue", "affiliation": "Universidad Nacional de C\u00f3rdoba", "sponsor": "Oracle Labs", "video": "https://www.youtube.com/watch?v=Sg9Z1NDyCZo", "abstract": "In 1991, Brennan asked: Why is it that natural language has yet to become a widely used modality of human/computer interaction? Almost 30 years later de Vries (2020) and colleagues ask the same question again. In this talk I will argue that a fundamental limitation of dialogue systems is that they cannot ground language into the world in a human-like way. I will distinguish two types of different but related kinds of grounding: perceptual and collaborative.\u00a0 First, I will describe our work on perceptual grounding [1]. I will show recent advances on referential dialogue adapting multimodal architectures for visual question answering. I will discuss the limitations of treating dialogue as question answering.\u00a0\u00a0 Second, I will present a recent opinion piece and state of the art survey on collaborative grounding [2]. Our central claim is that current dialogue systems try to avoid mistakes at all costs and this approach misses the key point that errors are a crucial mechanism in dialogue; for it is the ability to recover from them that makes dialogue such a robust process.\u00a0 [1] Answering Different Visual Questions Requires Different Grounding Strategies. Alberto Testoni, Claudio Greco, Tobias Bianchi, Mauricio Mazuecos, Agata Marcante, Raffaella Bernardi. Proceedings of the International Workshop on Spatial Language Understanding. EMNLP 2020. https://www.aclweb.org/anthology/2020.splu-1.4/ [2] Grounding as a Collaborative Process. Luciana Benotti & Patrick Blackburn. Long paper accepted at the 16th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2021. 19 to 23 April 2021. ", "bio": "Luciana Benotti is an Associate Professor at the Department of Computer Science in the Universidad Nacional de C\u00f3rdoba, from Argentina. Her research interests include different aspects of situated dialogue systems, including the study of misunderstandings, clarification requests and grounding. She has an Erasmus Mundus MSc, and a PhD in Computer Science completed at INRIA Nancy Grand Est in France. She received an IBM SUR award for her work on robust conversational interfaces, and a Google RISE award for her outreach efforts in developing AI-based technology for education. She has been an invited scholar at the University of Trento (2019), Stanford University (2018), Roskilde University (2014), University of Lorraine (2013), Universidad de Costa Rica (2012), and University of Southern California (2010). She regularly serves under different roles in the Association for Computational Linguistics (ACL) community. She has been a volunteer during conferences, a reviewer since 2010, an area chair for dialogue and interactive systems several times, and a member of the executive board of SIGDIAL and SIGSEM. She is currently an elected member of the executive board of the North American Chapter of the ACL. ", "area": "NLP", "key": "LucianaBenotti", "prettyDate": "April 15"}, {"semester": "Spring", "year": "2021", "date": "2021-04-29", "speaker": "Adam Dziedzic", "website": "https://adam-dziedzic.github.io/", "title": "CaPC Learning: Confidential and Private Collaborative Learning", "affiliation": "the Vector Institute and the University of Toronto", "sponsor": "Oracle Labs", "video": "https://www.youtube.com/watch?v=YA2DxGfRMKg", "abstract": "Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties.", "bio": "Adam is a postdoctoral researcher at the Vector Institute and the University of Toronto, advised by Prof. Nicolas Papernot. He earned his PhD at the University of Chicago, where he was advised by Prof. Sanjay Krishnan and carried out research on the Band-Limited convolutional neural networks as well as the out-of-distribution robustness of pre-trained transformers. Adam obtained his Bachelor's and Master's degrees from Warsaw University of Technology in Poland. He also studied at DTU (Technical University of Denmark) and carried out research on databases in the DIAS group at EPFL, Switzerland. He was a PhD intern at Microsoft Research and worked on recommendation of hybrid physical designs (B+ trees and Columnstores) for SQL Server. He also had internships at CERN (Geneva, Switzerland), Barclays Investment Bank (London, UK), and Google (Madison, USA).", "area": "ML security", "key": "AdamDziedzic", "prettyDate": "April 29"}]}